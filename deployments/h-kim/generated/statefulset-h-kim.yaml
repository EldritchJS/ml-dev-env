apiVersion: v1
kind: Service
metadata:
  name: h-kim-headless
  namespace: nccl-test
  labels:
    app: h-kim-multi
spec:
  clusterIP: None  # Headless service
  selector:
    app: h-kim-multi
  ports:
  - port: 29500
    name: master
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: h-kim
  namespace: nccl-test
  labels:
    app: h-kim-multi
spec:
  serviceName: h-kim-headless
  replicas: 2  # 2 nodes = 8 GPUs total
  podManagementPolicy: Parallel

  selector:
    matchLabels:
      app: h-kim-multi

  template:
    metadata:
      labels:
        app: h-kim-multi

    spec:
      restartPolicy: Always

      # Init container to auto-detect RDMA interfaces
      initContainers:
      - name: detect-rdma
        image: image-registry.openshift-image-registry.svc:5000/nccl-test/h-kim:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Detecting RDMA interfaces..."

          # Detect InfiniBand devices
          if command -v ibv_devinfo &> /dev/null; then
            IB_DEVICES=$(ibv_devinfo -l 2>/dev/null | grep -v "^$" | tr '\n' ',' | sed 's/,$//' || echo "")
            if [ -n "$IB_DEVICES" ]; then
              echo "export NCCL_IB_HCA=\"$IB_DEVICES\"" >> /shared/nccl-env.sh
              echo "Detected IB devices: $IB_DEVICES"
            else
              echo "# No IB devices detected" >> /shared/nccl-env.sh
              echo "Warning: No IB devices found"
            fi
          else
            echo "# ibv_devinfo not available" >> /shared/nccl-env.sh
            echo "Warning: ibv_devinfo not available"
          fi

          # Detect RDMA network interfaces (net1, net2, net3, net4)
          RDMA_IFACES=$(ip -o link show | awk -F': ' '{print $2}' | grep -E '^net[0-9]+$' | tr '\n' ',' | sed 's/,$//' || echo "eth0")
          if [ -n "$RDMA_IFACES" ]; then
            echo "export NCCL_SOCKET_IFNAME=\"$RDMA_IFACES\"" >> /shared/nccl-env.sh
            echo "Detected RDMA interfaces: $RDMA_IFACES"
          else
            echo "export NCCL_SOCKET_IFNAME=\"eth0\"" >> /shared/nccl-env.sh
            echo "Warning: No RDMA interfaces found, using eth0"
          fi

          echo "RDMA detection complete"
          cat /shared/nccl-env.sh
        volumeMounts:
        - name: nccl-env
          mountPath: /shared

      # Spread pods across different nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - moc-r4pcc04u23-nairr
                - moc-r4pcc04u25-nairr
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - h-kim-multi
            topologyKey: kubernetes.io/hostname

      nodeSelector:
        nvidia.com/gpu.present: "true"

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      containers:
      - name: h-kim
        image: image-registry.openshift-image-registry.svc:5000/nccl-test/h-kim:latest
        imagePullPolicy: Always

        resources:
          requests:
            nvidia.com/gpu: 4
            memory: 128Gi
            cpu: 32
          limits:
            nvidia.com/gpu: 4
            memory: 256Gi
            cpu: 64

        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"

        # NCCL configuration for RDMA (devices auto-detected by init container)
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "0"
        # NCCL_IB_HCA: auto-detected by init container
        - name: NCCL_IB_GID_INDEX
          value: "3"
        - name: NCCL_NET_GDR_LEVEL
          value: "5"
        # NCCL_SOCKET_IFNAME: auto-detected by init container

        # Pod identity
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

        # Distributed training settings
        - name: MASTER_ADDR
          value: "h-kim-0.h-kim-headless.nccl-test.svc.cluster.local"
        - name: MASTER_PORT
          value: "29500"
        - name: WORLD_SIZE
          value: "8"  # 2 nodes Ã— 4 GPUs
        - name: GPUS_PER_NODE
          value: "4"

        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: dshm
          mountPath: /dev/shm
        - name: nccl-env
          mountPath: /shared

        ports:
        - containerPort: 29500
          name: master
          protocol: TCP

        command:
        - /bin/bash
        - -c
        - |
          set -e

          # Source auto-detected RDMA configuration
          if [ -f /shared/nccl-env.sh ]; then
            echo "Loading auto-detected RDMA configuration..."
            source /shared/nccl-env.sh
            echo "NCCL_IB_HCA=$NCCL_IB_HCA"
            echo "NCCL_SOCKET_IFNAME=$NCCL_SOCKET_IFNAME"
          else
            echo "Warning: /shared/nccl-env.sh not found, using defaults"
          fi
          echo ""

          # Calculate node rank from pod ordinal
          POD_ORDINAL=${HOSTNAME##*-}
          export NODE_RANK=$POD_ORDINAL

          echo "=========================================="
          echo "H-Kim Multi-Node Environment"
          echo "=========================================="
          echo "Pod: $POD_NAME"
          echo "Node Rank: $NODE_RANK"
          echo "World Size: $WORLD_SIZE"
          echo "Master: $MASTER_ADDR:$MASTER_PORT"
          echo ""

          # Show NCCL configuration
          echo "NCCL Configuration:"
          env | grep NCCL_ | sort
          echo ""

          # Show GPU info
          nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader
          echo ""

          echo "Environment ready. Waiting for training job..."
          sleep infinity

      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: nccl-env
        emptyDir: {}

  # Each pod gets its own workspace PVC
  volumeClaimTemplates:
  - metadata:
      name: workspace
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
      storageClassName: ocs-external-storagecluster-ceph-rbd
