apiVersion: v1
kind: Service
metadata:
  name: h-kim-headless
  namespace: b-efficient-memory-offloading-765cab
  labels:
    app: h-kim-multi
spec:
  clusterIP: None  # Headless service
  selector:
    app: h-kim-multi
  ports:
  - port: 29500
    name: master
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: h-kim
  namespace: b-efficient-memory-offloading-765cab
  labels:
    app: h-kim-multi
spec:
  serviceName: h-kim-headless
  replicas: 2  # Scale up to 6 for full cluster (24 GPUs)
  podManagementPolicy: Parallel

  selector:
    matchLabels:
      app: h-kim-multi

  template:
    metadata:
      labels:
        app: h-kim-multi
      annotations:
        k8s.v1.cni.cncf.io/networks: default/eno5np0-network,default/eno6np0-network,default/eno7np0-network,default/eno8np0-network

    spec:
      restartPolicy: Always

      # Init container for comprehensive NCCL auto-detection
      # Note: Init containers cannot request GPUs (would conflict with main container)
      # Therefore NVLink and IB device detection may be limited
      # Users can override NCCL_P2P_LEVEL and NCCL_IB_HCA via env vars if needed
      initContainers:
      - name: autodetect-nccl
        image: image-registry.openshift-image-registry.svc:5000/b-efficient-memory-offloading-765cab/h-kim:latest

        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "=========================================="
          echo "Comprehensive NCCL Auto-Detection"
          echo "=========================================="
          echo ""
          echo "Note: Init containers cannot access GPUs/IB devices directly"
          echo "NVLink and IB HCA detection are conservative defaults"
          echo "Override via env vars if needed (see AUTODETECT-OVERRIDES.md)"
          echo ""

          # Run the comprehensive autodetect script
          /usr/local/bin/autodetect-nccl.sh

          echo ""
          echo "=========================================="
          echo "Auto-Detected Configuration:"
          echo "=========================================="
          cat /shared/nccl-env.sh
          echo "=========================================="
        volumeMounts:
        - name: nccl-env
          mountPath: /shared
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RESOURCE

      # Spread pods across different nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - moc-r4pcc04u09-nairr
                - moc-r4pcc04u11-nairr
                - moc-r4pcc04u12-nairr
                - moc-r4pcc04u16-nairr
                - moc-r4pcc04u25-nairr
                - moc-r4pcc04u36-nairr
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - h-kim-multi
            topologyKey: kubernetes.io/hostname

      nodeSelector:
        nvidia.com/gpu.present: "true"

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      # Service account with nccl-rdma-scc for unlimited memlock
      serviceAccountName: h-kim-sa

      containers:
      - name: h-kim
        image: image-registry.openshift-image-registry.svc:5000/b-efficient-memory-offloading-765cab/h-kim:latest
        imagePullPolicy: Always

        # Security context for RDMA - SYS_RESOURCE allows setting unlimited memlock
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
              - SYS_RESOURCE

        resources:
          requests:
            nvidia.com/gpu: 4
            openshift.io/eno5np0rdma: 1
            openshift.io/eno6np0rdma: 1
            openshift.io/eno7np0rdma: 1
            openshift.io/eno8np0rdma: 1
            memory: 1200Gi
            cpu: 32
          limits:
            nvidia.com/gpu: 4
            openshift.io/eno5np0rdma: 1
            openshift.io/eno6np0rdma: 1
            openshift.io/eno7np0rdma: 1
            openshift.io/eno8np0rdma: 1
            memory: 1200Gi
            cpu: 64

        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"

        # NCCL configuration (all values auto-detected by init container)
        # Users can override any value by setting env vars here
        - name: NCCL_DEBUG
          value: "INFO"
        # Override for SR-IOV networking
        - name: NCCL_SOCKET_IFNAME
          value: "net1,net2,net3,net4"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        # Let NCCL auto-detect IB HCA based on SOCKET_IFNAME
        - name: NCCL_IB_HCA
          value: ""
        # Use Ring algorithm for consistency
        - name: NCCL_ALGO
          value: "Ring"
        # All other NCCL values auto-detected:
        # - NCCL_IB_DISABLE (auto)
        # - NCCL_IB_GID_INDEX (auto)
        # - NCCL_NET_GDR_LEVEL (auto)
        # - NCCL_P2P_LEVEL (auto)
        # - GPUS_PER_NODE (auto)
        # - OMP_NUM_THREADS (auto)

        # Pod identity
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

        # Distributed training settings
        - name: MASTER_ADDR
          value: "h-kim-0.h-kim-headless.b-efficient-memory-offloading-765cab.svc.cluster.local"
        - name: MASTER_PORT
          value: "29500"
        # GPUS_PER_NODE: auto-detected by init container
        # Note: Specify --nnodes when running torchrun (2, 6, etc.)

        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: dshm
          mountPath: /dev/shm
        - name: nccl-env
          mountPath: /shared

        ports:
        - containerPort: 29500
          name: master
          protocol: TCP

        command:
        - /bin/bash
        - -c
        - |
          set -e

          # Set unlimited memlock for RDMA (works with SYS_RESOURCE capability)
          ulimit -l unlimited
          echo "Memlock limit: $(ulimit -l)"
          echo ""

          # Source comprehensive auto-detected configuration
          if [ -f /shared/nccl-env.sh ]; then
            echo "=========================================="
            echo "Loading Auto-Detected Configuration"
            echo "=========================================="
            source /shared/nccl-env.sh
            echo ""
          else
            echo "ERROR: /shared/nccl-env.sh not found!"
            echo "Autodetect init container may have failed"
            exit 1
          fi

          # Calculate node rank from pod ordinal
          POD_ORDINAL=${HOSTNAME##*-}
          export NODE_RANK=$POD_ORDINAL

          echo "=========================================="
          echo "H-Kim Multi-Node Environment"
          echo "=========================================="
          echo "Pod: $POD_NAME"
          echo "Node Rank: $NODE_RANK"
          echo ""
          echo "AUTO-DETECTED VALUES:"
          echo "  GPUs per node: $GPUS_PER_NODE (detected!)"
          echo "  OMP threads: $OMP_NUM_THREADS (detected!)"
          echo "  Transport: $DETECTED_TRANSPORT (detected!)"
          echo ""
          echo "Master: $MASTER_ADDR:$MASTER_PORT"
          echo "=========================================="
          echo ""

          # Show comprehensive NCCL configuration
          echo "NCCL Configuration (auto-detected):"
          env | grep NCCL_ | sort
          echo ""

          # Show GPU topology
          echo "GPU Topology:"
          nvidia-smi topo -m 2>/dev/null || echo "  (topology info not available)"
          echo ""

          # Show detected GPUs
          echo "Detected GPUs:"
          nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader
          echo ""

          echo "Environment ready. Waiting for training job..."
          echo "To run training:"
          echo "  oc exec -it $POD_NAME -- bash"
          echo "  cd /workspace"
          echo "  torchrun --nnodes=<NUM_NODES> --nproc_per_node=\$GPUS_PER_NODE --node_rank=\$NODE_RANK \\"
          echo "    --master_addr=\$MASTER_ADDR --master_port=\$MASTER_PORT train.py"
          echo ""
          echo "Note: Use 'oc scale statefulset h-kim --replicas=N' to change node count"
          echo ""

          sleep infinity

      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: nccl-env
        emptyDir: {}

  # Each pod gets its own workspace PVC
  volumeClaimTemplates:
  - metadata:
      name: workspace
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
      storageClassName: ocs-external-storagecluster-ceph-rbd
