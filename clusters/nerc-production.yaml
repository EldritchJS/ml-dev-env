# NERC Production Cluster Configuration
# Cluster: shift.nerc.mghpcc.org

cluster:
  name: nerc-production
  api: api.shift.nerc.mghpcc.org
  namespace: coops-767192
  description: "NERC Production cluster with H100 GPUs"

# Node configuration
nodes:
  # Specific GPU nodes to use (leave empty for auto-selection)
  gpu_nodes:
  # Optional: Add more nodes as needed
  # -
  # -

# Storage configuration
storage:
  # ReadWriteMany storage class (for shared workspace)
  class_rwx: nfs-csi
  # ReadWriteOnce storage class (for individual pods)
  class_rwo: ocs-external-storagecluster-ceph-rbd
  # Storage sizes
  workspace_size: 100Gi
  datasets_size: 500Gi
  # Storage mode: "rwx" for shared storage, "volumeClaimTemplates" for per-pod
  # Note: This cluster has NFS server running, RWX storage available
  mode: rwx

# Network configuration
network:
  # RDMA/InfiniBand settings
  rdma:
    enabled: false
    # Mellanox devices (original configuration from Barcelona)
    devices: "mlx5_6,mlx5_7,mlx5_10,mlx5_11"
    # Network interfaces for RDMA
    interfaces: "net1,net2,net3,net4"
    # GID index for RoCE v2
    gid_index: "3"
    # GPUDirect RDMA level
    gdr_level: "5"
    # NCCL tuning
    cross_nic: "1"
    ib_timeout: "22"
    min_nchannels: "4"
  # TCP/Ethernet fallback settings
  tcp:
    # Interfaces to exclude (use primary interface)
    interface_exclude: "^lo,docker0"
    # P2P level (NVL = NVLink intra-node, TCP inter-node)
    p2p_level: "NVL"

# Security configuration
security:
  # Service account name
  service_account: ml-dev-sa
  # Whether privileged SCC is required (may vary by cluster policy)
  requires_privileged_scc: false
  # Enable IPC_LOCK capability (may not be allowed without privileged SCC)
  ipc_lock: false

# GPU configuration
gpus:
  # GPUs per node
  per_node: 4
  # GPU type
  type: "NVIDIA H100 80GB HBM3"
  # Total nodes for multi-node (adjust as needed)
  default_nodes: 2

# Resource limits per pod
resources:
  requests:
    memory: 128Gi
    cpu: 32
  limits:
    memory: 256Gi
    cpu: 64

# NCCL debug level
nccl:
  debug: "INFO"

# Notes
notes: |
  NERC Production cluster (shift) configuration.

  Verified cluster details:
  - 25 GPU nodes available (wrk-97 through wrk-128)
  - 4x H100 80GB HBM3 per node
  - NFS server running (nfs-csi storage class available)
  - RWX shared storage via NFS
  - No RDMA/InfiniBand devices detected (TCP mode only)

  Setup steps:
  1. Create service account:
     oc create serviceaccount ml-dev-sa -n coops-767192

  2. If privileged SCC needed (for IPC_LOCK):
     oc adm policy add-scc-to-user privileged -z ml-dev-sa -n coops-767192

  3. Verify NFS server is running:
     oc get pods -n nfs
