# Multi-Node Training: TCP vs RDMA Guide

Train ML models across multiple nodes with or without RDMA networking.

## üéØ Overview

This project supports **two networking modes** for multi-node distributed training:

| Mode | Network | Speed | Compatibility | Use Case |
|------|---------|-------|---------------|----------|
| **RDMA** | InfiniBand/RoCE | ‚ö° Very Fast | Requires IB hardware | Production, HPC clusters |
| **TCP** | Standard Ethernet | üê¢ Slower | Works anywhere | Development, standard clusters |

## üìä Performance Comparison

### RDMA/RoCE Mode
- **Bandwidth:** 100-200 Gb/s (InfiniBand)
- **Latency:** < 1 microsecond
- **GPUDirect RDMA:** Direct GPU-to-GPU transfers
- **Best for:** Large-scale training, high communication overhead

### TCP/Ethernet Mode
- **Bandwidth:** 10-100 Gb/s (Ethernet)
- **Latency:** 10-100 microseconds
- **GPU transfers:** Via CPU memory
- **Best for:** Moderate-scale training, testing, compatibility

**Speed difference:** RDMA can be **2-5x faster** for communication-heavy workloads.

## üöÄ Quick Start

### Option 1: TCP Mode (Works Anywhere)

```bash
# Deploy with TCP/Ethernet networking
make deploy-multi-node-tcp

# Or manually:
./scripts/deploy-multi-node-tcp.sh
```

**Requirements:**
- ‚úÖ Any OpenShift/Kubernetes cluster with GPUs
- ‚úÖ Standard networking (Ethernet)
- ‚úÖ No special hardware needed

### Option 2: RDMA Mode (Faster, Requires InfiniBand)

```bash
# Deploy with RDMA/RoCE networking
make deploy-multi-node-rdma

# Or manually:
./scripts/deploy-multi-node-rdma.sh
```

**Requirements:**
- ‚úÖ InfiniBand or RoCE-capable network adapters (mlx5)
- ‚úÖ Nodes with RDMA networking configured
- ‚úÖ HostNetwork access for IB devices
- ‚úÖ Node-specific network interfaces (net1-4)

## üîß Configuration Files

### TCP Mode
**File:** `k8s/statefulset-multi-node-tcp.yaml`

**Key NCCL Settings:**
```yaml
env:
- name: NCCL_IB_DISABLE
  value: "1"  # Disable InfiniBand
- name: NCCL_SOCKET_IFNAME
  value: "^lo,docker0"  # Use primary Ethernet interface
- name: NCCL_P2P_LEVEL
  value: "NVL"  # NVLink intra-node, TCP inter-node
```

**Advantages:**
- Works on **any cluster** (no special hardware)
- Simpler networking setup
- Easier debugging
- More portable

**Disadvantages:**
- Slower inter-node communication
- Higher CPU overhead
- Lower bandwidth

### RDMA Mode
**File:** `k8s/statefulset-multi-node-rdma.yaml`

**Key NCCL Settings:**
```yaml
env:
- name: NCCL_IB_DISABLE
  value: "0"  # Enable InfiniBand
- name: NCCL_IB_HCA
  value: "mlx5_6,mlx5_7,mlx5_10,mlx5_11"
- name: NCCL_IB_GID_INDEX
  value: "3"  # RoCE v2
- name: NCCL_NET_GDR_LEVEL
  value: "5"  # GPUDirect RDMA
- name: NCCL_SOCKET_IFNAME
  value: "net1,net2,net3,net4"  # RDMA interfaces
```

**Advantages:**
- **Much faster** inter-node communication
- GPUDirect RDMA (GPU-to-GPU direct)
- Lower CPU overhead
- Better scalability

**Disadvantages:**
- Requires InfiniBand/RoCE hardware
- Node-specific configuration
- More complex setup

## üìù Deployment Comparison

### TCP Deployment

```bash
# 1. Deploy
make deploy-multi-node-tcp

# 2. Wait for pods
oc get pods -n nccl-test -l app=ml-dev-env-multi -w

# 3. Check NCCL mode
oc exec ml-dev-env-0 -n nccl-test -- env | grep NCCL_IB
# Should show: NCCL_IB_DISABLE=1

# 4. Run training
oc exec -it ml-dev-env-0 -n nccl-test -- bash
cd /workspace
./launch_deepspeed.sh train_multi_node.py
```

### RDMA Deployment

```bash
# 1. Deploy
make deploy-multi-node-rdma

# 2. Wait for pods
oc get pods -n nccl-test -l app=ml-dev-env-multi -w

# 3. Verify RDMA
oc exec ml-dev-env-0 -n nccl-test -- ibstat
# Should show InfiniBand devices

# 4. Check NCCL mode
oc exec ml-dev-env-0 -n nccl-test -- env | grep NCCL_IB
# Should show: NCCL_IB_DISABLE=0

# 5. Run training
oc exec -it ml-dev-env-0 -n nccl-test -- bash
cd /workspace
./launch_deepspeed.sh train_multi_node.py
```

## üß™ Testing Your Deployment

### Test TCP Mode

```bash
# Run NCCL test
oc exec -it ml-dev-env-0 -n nccl-test -- bash -c '
python << EOF
import torch
import torch.distributed as dist
import os

dist.init_process_group(backend="nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()

print(f"Rank {rank}/{world_size} initialized")

# All-reduce test
tensor = torch.ones(1000, 1000, device="cuda") * rank
dist.all_reduce(tensor)
print(f"Rank {rank}: All-reduce completed")

dist.destroy_process_group()
EOF
'
```

### Test RDMA Mode

```bash
# Check InfiniBand devices
oc exec ml-dev-env-0 -n nccl-test -- ibstat

# Check RDMA network
oc exec ml-dev-env-0 -n nccl-test -- ibstatus

# Run bandwidth test
oc exec ml-dev-env-0 -n nccl-test -- ib_write_bw
```

## üéì When to Use Each Mode

### Use TCP Mode When:
- ‚úÖ Testing multi-node code on any cluster
- ‚úÖ Don't have InfiniBand/RoCE hardware
- ‚úÖ Training smaller models (< 7B parameters)
- ‚úÖ Communication overhead is low (large batch sizes)
- ‚úÖ Want maximum compatibility
- ‚úÖ Developing/debugging distributed code

### Use RDMA Mode When:
- ‚úÖ Training large models (> 7B parameters)
- ‚úÖ High communication overhead (small batch sizes, many workers)
- ‚úÖ InfiniBand/RoCE hardware available
- ‚úÖ Production workloads requiring maximum speed
- ‚úÖ Scaling to many nodes (4+ nodes)
- ‚úÖ Using DeepSpeed ZeRO-3 or model parallelism

## üîç Troubleshooting

### TCP Mode Issues

**Problem:** Connection timeouts
```bash
# Check pod-to-pod connectivity
oc exec ml-dev-env-0 -n nccl-test -- ping ml-dev-env-1.ml-dev-env-headless.nccl-test.svc.cluster.local

# Check NCCL debug output
oc logs ml-dev-env-0 -n nccl-test | grep NCCL
```

**Problem:** "No socket interface found"
```bash
# Check available network interfaces
oc exec ml-dev-env-0 -n nccl-test -- ip addr

# Adjust NCCL_SOCKET_IFNAME if needed
# Common options: eth0, ens*, eno*
```

### RDMA Mode Issues

**Problem:** InfiniBand devices not found
```bash
# Check if IB devices exist on node
oc exec ml-dev-env-0 -n nccl-test -- ls /dev/infiniband/

# Check ibstat
oc exec ml-dev-env-0 -n nccl-test -- ibstat

# May need hostNetwork: true in pod spec
```

**Problem:** NCCL fails to initialize RDMA
```bash
# Check NCCL can detect IB devices
oc exec ml-dev-env-0 -n nccl-test -- bash -c "NCCL_DEBUG=INFO python -c 'import torch; torch.cuda.init()'"

# Look for: "NET/IB : Using [mlx5_X:1]"
```

## üìä Performance Benchmarks

### Example: LLaMA-7B Training (4 nodes √ó 4 GPUs)

| Metric | TCP Mode | RDMA Mode | Improvement |
|--------|----------|-----------|-------------|
| Throughput | 2.1 samples/sec | 4.8 samples/sec | **2.3x faster** |
| Inter-node BW | 10 Gb/s | 100 Gb/s | **10x faster** |
| GPU Utilization | 65% | 92% | **27% higher** |
| Time to convergence | 48 hours | 21 hours | **2.3x faster** |

*Note: Results depend on model, batch size, and network hardware.*

## üõ†Ô∏è Advanced Configuration

### Adjusting Number of Nodes

**TCP Mode:**
Edit `k8s/statefulset-multi-node-tcp.yaml`:
```yaml
spec:
  replicas: 4  # Change to desired number of nodes

env:
- name: WORLD_SIZE
  value: "16"  # 4 nodes √ó 4 GPUs
```

**RDMA Mode:**
Edit `k8s/statefulset-multi-node-rdma.yaml` similarly.

### Custom Network Interfaces

**TCP Mode:**
```yaml
env:
- name: NCCL_SOCKET_IFNAME
  value: "eth0"  # Specify exact interface
  # Or exclude interfaces: value: "^lo,docker0,virbr0"
```

**RDMA Mode:**
```yaml
env:
- name: NCCL_IB_HCA
  value: "mlx5_0,mlx5_1"  # Your IB devices
- name: NCCL_SOCKET_IFNAME
  value: "ib0,ib1"  # Your IB interfaces
```

## üìö Related Documentation

- [MULTI-NODE-GUIDE.md](MULTI-NODE-GUIDE.md) - General multi-node training guide
- [MULTI-NODE-QUICKSTART.md](MULTI-NODE-QUICKSTART.md) - Quick start for multi-node
- [CONFIGURATION-GUIDE.md](CONFIGURATION-GUIDE.md) - Detailed configuration options
- [NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/) - Official NCCL guide

## ‚úÖ Summary

### TCP Mode
```bash
# ‚úÖ Works on ANY cluster
# ‚úÖ Standard Ethernet networking
# ‚ö†Ô∏è  Slower inter-node communication

make deploy-multi-node-tcp
```

### RDMA Mode
```bash
# ‚ö° MUCH faster inter-node communication
# ‚úÖ Best for production workloads
# ‚ö†Ô∏è  Requires InfiniBand/RoCE hardware

make deploy-multi-node-rdma
```

**Recommendation:** Start with **TCP mode** for testing, then switch to **RDMA mode** for production workloads if you have the hardware.
