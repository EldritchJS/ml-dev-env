apiVersion: v1
kind: Pod
metadata:
  name: deepti-test
  namespace: nccl-test
  labels:
    app: deepti-test
    workload: qwen-omni
spec:
  restartPolicy: Never

  # Node selector to ensure placement on GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"

  # Optional: Pin to specific node
  # Uncomment to run on specific Barcelona node
  # nodeName: moc-r4pcc04u25-nairr

  # Service account for permissions
  serviceAccountName: ml-dev-sa

  containers:
  - name: qwen-omni
    image: image-registry.openshift-image-registry.svc:5000/nccl-test/ml-dev-env:latest
    imagePullPolicy: Always

    # Request 4 GPUs (single node)
    resources:
      requests:
        nvidia.com/gpu: 4
        memory: 128Gi
        cpu: 32
      limits:
        nvidia.com/gpu: 4
        memory: 256Gi
        cpu: 64

    # Environment variables for multi-GPU (Barcelona cluster)
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
    - name: CUDA_VISIBLE_DEVICES
      value: "0,1,2,3"
    - name: NCCL_DEBUG
      value: "INFO"
    # Barcelona has RDMA - enable InfiniBand
    - name: NCCL_IB_DISABLE
      value: "0"
    - name: NCCL_IB_HCA
      value: "mlx5_6,mlx5_7,mlx5_10,mlx5_11"
    - name: NCCL_IB_GID_INDEX
      value: "3"
    - name: NCCL_NET_GDR_LEVEL
      value: "5"
    - name: NCCL_SOCKET_IFNAME
      value: "net1,net2,net3,net4"
    - name: NCCL_P2P_LEVEL
      value: "NVL"
    - name: OMP_NUM_THREADS
      value: "8"

    # Working directory
    workingDir: /workspace

    # Startup command
    command:
    - /bin/bash
    - -c
    - |
      set -e

      echo "=========================================="
      echo "Qwen2.5-Omni Test (Barcelona Cluster)"
      echo "=========================================="
      echo ""

      # Show GPU information
      echo "GPU Information:"
      nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
      echo ""

      echo "GPU Topology:"
      nvidia-smi topo -m
      echo ""

      # Show InfiniBand devices
      echo "InfiniBand Devices:"
      ibstat 2>/dev/null || echo "  (ibstat not available, but RDMA may still work)"
      echo ""

      # Verify qwen-omni-utils is installed
      echo "Verifying Python environment:"
      python -c "import torch; print(f'PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
      python -c "import transformers; print(f'Transformers {transformers.__version__}')"
      python -c "from qwen_omni_utils import process_mm_info; print('qwen-omni-utils: installed')"
      python -c "import flash_attn; print(f'flash-attn: {flash_attn.__version__}')"
      echo ""

      # Copy deepti.py from ConfigMap to workspace
      if [ -f /config/deepti.py ]; then
        cp /config/deepti.py /workspace/deepti.py
        echo "Copied deepti.py to workspace"
      else
        echo "ERROR: deepti.py not found in ConfigMap"
        exit 1
      fi

      echo "=========================================="
      echo "Running deepti.py..."
      echo "=========================================="
      echo ""

      # Run the test script
      cd /workspace
      python deepti.py

      echo ""
      echo "=========================================="
      echo "Test completed successfully!"
      echo "=========================================="

    # Volume mounts
    volumeMounts:
    - name: deepti-script
      mountPath: /config
      readOnly: true

  volumes:
  - name: deepti-script
    configMap:
      name: deepti-script

  # Tolerate GPU node taints
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
