apiVersion: v1
kind: Pod
metadata:
  name: deepti-test
  namespace: coops-767192
  labels:
    app: deepti-test
    workload: qwen-omni
spec:
  restartPolicy: Never

  # Node selector to ensure placement on GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"

  # Service account for permissions
  serviceAccountName: ml-dev-sa

  containers:
  - name: qwen-omni
    # Using NVIDIA PyTorch base image directly (no custom build needed)
    image: nvcr.io/nvidia/pytorch:24.10-py3
    imagePullPolicy: IfNotPresent

    # Request 4 GPUs (single node)
    resources:
      requests:
        nvidia.com/gpu: 4
        memory: 128Gi
        cpu: 32
      limits:
        nvidia.com/gpu: 4
        memory: 256Gi
        cpu: 64

    # Environment variables for multi-GPU
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
    - name: CUDA_VISIBLE_DEVICES
      value: "0,1,2,3"
    - name: NCCL_DEBUG
      value: "INFO"
    # NERC doesn't have RDMA, use TCP
    - name: NCCL_IB_DISABLE
      value: "1"
    - name: NCCL_SOCKET_IFNAME
      value: "^lo,docker0"
    - name: NCCL_P2P_LEVEL
      value: "NVL"
    - name: OMP_NUM_THREADS
      value: "8"

    # Working directory
    workingDir: /workspace

    # Startup command
    command:
    - /bin/bash
    - -c
    - |
      set -e

      echo "=========================================="
      echo "Qwen2.5-Omni Test Setup (NERC Cluster)"
      echo "=========================================="
      echo ""

      # Show GPU information
      echo "GPU Information:"
      nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
      echo ""

      echo "GPU Topology:"
      nvidia-smi topo -m
      echo ""

      # Install system dependencies
      echo "Installing system dependencies (ffmpeg)..."
      apt-get update -qq && apt-get install -y -qq ffmpeg
      echo ""

      # Install Python dependencies
      echo "Installing Python dependencies..."
      pip install --no-cache-dir --upgrade pip setuptools wheel
      echo "  - Installing transformers..."
      pip install --no-cache-dir transformers>=4.37.0 accelerate
      echo "  - Installing flash-attention..."
      pip install --no-cache-dir packaging ninja flash-attn --no-build-isolation
      echo "  - Installing qwen-omni-utils..."
      pip install --no-cache-dir qwen-omni-utils -U
      echo ""

      # Verify installation
      echo "Python environment:"
      python -c "import torch; print(f'PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
      python -c "import transformers; print(f'Transformers {transformers.__version__}')"
      python -c "from qwen_omni_utils import process_mm_info; print('qwen-omni-utils: installed')"
      echo ""

      # Copy deepti.py from ConfigMap to workspace
      if [ -f /config/deepti.py ]; then
        cp /config/deepti.py /workspace/deepti.py
        echo "Copied deepti.py to workspace"
      fi

      echo "=========================================="
      echo "Running deepti.py..."
      echo "=========================================="
      echo ""

      # Run the test script
      cd /workspace
      python deepti.py

      echo ""
      echo "=========================================="
      echo "Test completed!"
      echo "=========================================="

    # Volume mounts
    volumeMounts:
    - name: deepti-script
      mountPath: /config
      readOnly: true

  volumes:
  - name: deepti-script
    configMap:
      name: deepti-script

  # Tolerate GPU node taints
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
