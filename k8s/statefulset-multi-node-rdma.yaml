apiVersion: v1
kind: Service
metadata:
  name: {app_name}-headless
  namespace: {namespace}
  labels:
    app: {app_name}-multi
spec:
  clusterIP: None  # Headless service for StatefulSet
  selector:
    app: {app_name}-multi
  ports:
  - port: 29500
    name: master
  - port: 8080
    name: code-server
  - port: 5678
    name: debug
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {app_name}
  namespace: {namespace}
  labels:
    app: {app_name}-multi
spec:
  serviceName: {app_name}-headless
  # CUSTOMIZE: Change replicas to set number of nodes (each node gets 1 pod)
  # Examples: 2 nodes = 8 GPUs, 4 nodes = 16 GPUs, 8 nodes = 32 GPUs
  replicas: 2  # Default: 2 nodes
  podManagementPolicy: Parallel  # Launch all pods simultaneously

  selector:
    matchLabels:
      app: {app_name}-multi

  template:
    metadata:
      labels:
        app: {app_name}-multi
        pytorch-version: "2.9"
        numpy-version: "2.2.6"

    spec:
      restartPolicy: Always

      # Init container to auto-detect RDMA interfaces
      initContainers:
      - name: detect-rdma
        image: image-registry.openshift-image-registry.svc:5000/nccl-test/ml-dev-env:pytorch-2.9-numpy2
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Detecting RDMA interfaces..."

          # Detect InfiniBand devices
          if command -v ibv_devinfo &> /dev/null; then
            IB_DEVICES=$(ibv_devinfo -l 2>/dev/null | grep -v "^$" | tr '\n' ',' | sed 's/,$//' || echo "")
            if [ -n "$IB_DEVICES" ]; then
              echo "export NCCL_IB_HCA=\"$IB_DEVICES\"" >> /shared/nccl-env.sh
              echo "Detected IB devices: $IB_DEVICES"
            else
              echo "# No IB devices detected" >> /shared/nccl-env.sh
              echo "Warning: No IB devices found"
            fi
          else
            echo "# ibv_devinfo not available" >> /shared/nccl-env.sh
            echo "Warning: ibv_devinfo not available"
          fi

          # Detect RDMA network interfaces (net1, net2, etc.)
          RDMA_IFACES=$(ip -o link show | awk -F': ' '{print $2}' | grep -E '^net[0-9]+$' | tr '\n' ',' | sed 's/,$//' || echo "eth0")
          if [ -n "$RDMA_IFACES" ]; then
            echo "export NCCL_SOCKET_IFNAME=\"$RDMA_IFACES\"" >> /shared/nccl-env.sh
            echo "Detected RDMA interfaces: $RDMA_IFACES"
          else
            echo "export NCCL_SOCKET_IFNAME=\"eth0\"" >> /shared/nccl-env.sh
            echo "Warning: No RDMA interfaces found, using eth0"
          fi

          echo "RDMA detection complete"
          cat /shared/nccl-env.sh
        volumeMounts:
        - name: nccl-env
          mountPath: /shared
        securityContext:
          capabilities:
            add:
              - IPC_LOCK

      # Spread pods across different nodes (one pod per node)
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - {app_name}-multi
            topologyKey: kubernetes.io/hostname

        # OPTIONAL: Constrain to specific nodes
        # Uncomment and edit to run on specific nodes:
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     nodeSelectorTerms:
        #     - matchExpressions:
        #       - key: kubernetes.io/hostname
        #         operator: In
        #         values:
        #         - your-node-1
        #         - your-node-2
        #         - your-node-3

      # Node selector to ensure placement on GPU nodes
      nodeSelector:
        nvidia.com/gpu.present: "true"

      # Tolerate GPU node taints
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      containers:
      - name: ml-dev
        image: image-registry.openshift-image-registry.svc:5000/nccl-test/ml-dev-env:pytorch-2.9-numpy2
        imagePullPolicy: Always

        # CUSTOMIZE: Change GPU count per pod
        # Total GPUs = replicas × GPUs per pod
        # Examples: 2 nodes × 4 GPUs = 8 total, 4 nodes × 8 GPUs = 32 total
        resources:
          requests:
            nvidia.com/gpu: 4  # Default: 4 GPUs per pod
            memory: 128Gi
            cpu: 32
          limits:
            nvidia.com/gpu: 4  # Must match requests
            memory: 256Gi
            cpu: 64

        # Security context for RDMA access
        securityContext:
          capabilities:
            add:
              - IPC_LOCK

        # Environment variables for multi-GPU and multi-node
        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"

        # NCCL configuration for multi-node (RDMA/RoCE - REQUIRES InfiniBand)
        - name: NCCL_DEBUG
          value: "INFO"
        # Enable InfiniBand/RDMA
        - name: NCCL_IB_DISABLE
          value: "0"
        # NCCL_IB_HCA: auto-detected by init container
        # NCCL_SOCKET_IFNAME: auto-detected by init container
        - name: NCCL_IB_GID_INDEX
          value: "3"
        - name: NCCL_NET_GDR_LEVEL
          value: "5"
        - name: NCCL_CROSS_NIC
          value: "1"
        - name: NCCL_IB_TIMEOUT
          value: "22"
        - name: NCCL_MIN_NCHANNELS
          value: "4"

        # Pod identity for DeepSpeed
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

        # DeepSpeed master address (pod-0 is master)
        - name: MASTER_ADDR
          value: "{app_name}-0.{app_name}-headless.{namespace}.svc.cluster.local"
        - name: MASTER_PORT
          value: "29500"

        # CUSTOMIZE: Update WORLD_SIZE and GPUS_PER_NODE to match your configuration
        # WORLD_SIZE = replicas × GPUS_PER_NODE (total GPUs across all nodes)
        - name: WORLD_SIZE
          value: "8"  # Default: 2 nodes × 4 GPUs = 8 total
        - name: GPUS_PER_NODE
          value: "4"  # Default: 4 GPUs per node (must match nvidia.com/gpu above)

        # OpenMP threads
        - name: OMP_NUM_THREADS
          value: "8"

        # Volume mounts
        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: datasets
          mountPath: /datasets
        - name: dshm
          mountPath: /dev/shm
        - name: nccl-env
          mountPath: /shared

        # Ports for services
        ports:
        - containerPort: 29500
          name: master
          protocol: TCP
        - containerPort: 8080
          name: code-server
          protocol: TCP
        - containerPort: 8888
          name: jupyter
          protocol: TCP
        - containerPort: 6006
          name: tensorboard
          protocol: TCP
        - containerPort: 5678
          name: debug
          protocol: TCP

        # Liveness probe
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - nvidia-smi
          initialDelaySeconds: 30
          periodSeconds: 30

        # Readiness probe
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - nvidia-smi && [ -d /workspace ]
          initialDelaySeconds: 10
          periodSeconds: 10

        # Startup command
        command:
        - /bin/bash
        - -c
        - |
          set -e

          # Source auto-detected RDMA configuration
          if [ -f /shared/nccl-env.sh ]; then
            echo "Loading auto-detected RDMA configuration..."
            source /shared/nccl-env.sh
            echo "NCCL_IB_HCA=$NCCL_IB_HCA"
            echo "NCCL_SOCKET_IFNAME=$NCCL_SOCKET_IFNAME"
          else
            echo "Warning: /shared/nccl-env.sh not found, using defaults"
          fi
          echo ""

          # Calculate node rank from pod ordinal
          POD_ORDINAL=${HOSTNAME##*-}
          export NODE_RANK=$POD_ORDINAL

          echo "=========================================="
          echo "ML Dev Environment - Multi-Node DeepSpeed"
          echo "Network Mode: RDMA/RoCE (InfiniBand)"
          echo "=========================================="
          echo "Pod: $POD_NAME"
          echo "Node Rank: $NODE_RANK"
          echo "World Size: $WORLD_SIZE"
          echo "Master: $MASTER_ADDR:$MASTER_PORT"
          echo ""

          # Show NCCL configuration
          echo "NCCL Configuration:"
          env | grep NCCL_ | sort
          echo ""

          # Show GPU info
          nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader
          echo ""

          # Start code-server in background
          code-server --bind-addr 0.0.0.0:8080 --auth none /workspace &

          # Create hostfile for DeepSpeed dynamically
          mkdir -p /workspace/.deepspeed
          NUM_NODES=$((WORLD_SIZE / GPUS_PER_NODE))
          > /workspace/.deepspeed/hostfile
          for i in $(seq 0 $((NUM_NODES - 1))); do
            echo "{app_name}-$i.{app_name}-headless.{namespace}.svc.cluster.local slots=$GPUS_PER_NODE" >> /workspace/.deepspeed/hostfile
          done

          echo "Hostfile created at /workspace/.deepspeed/hostfile ($NUM_NODES nodes)"
          cat /workspace/.deepspeed/hostfile
          echo ""
          echo "Ready for multi-node training!"
          echo "=========================================="

          # Application startup
          {app_startup_code}

          # Keep container running
          tail -f /dev/null

      volumes:
      - name: workspace
        persistentVolumeClaim:
          claimName: ml-dev-workspace
      - name: datasets
        persistentVolumeClaim:
          claimName: ml-datasets
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: nccl-env
        emptyDir: {}
