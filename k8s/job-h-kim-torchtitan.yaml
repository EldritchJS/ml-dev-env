apiVersion: v1
kind: ConfigMap
metadata:
  name: h-kim-training-script
  namespace: nccl-test
data:
  h-kim-openshift.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    # Adapted from h-kim.sh for OpenShift h-kim StatefulSet
    # This script runs TorchTitan distributed training on OpenShift

    # --- Logging / debugging ---
    export LOGLEVEL="${LOGLEVEL:-INFO}"
    export NCCL_DEBUG="${NCCL_DEBUG:-INFO}"
    export PYTHONFAULTHANDLER="${PYTHONFAULTHANDLER:-1}"
    export CUDA_LAUNCH_BLOCKING="${CUDA_LAUNCH_BLOCKING:-0}"

    # --- Networking (OpenShift with RDMA/InfiniBand) ---
    export NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-net1,net2,net3,net4}"

    # --- NCCL InfiniBand settings ---
    export NCCL_IB_DISABLE="${NCCL_IB_DISABLE:-0}"
    export NCCL_IB_HCA="${NCCL_IB_HCA:-mlx5_6,mlx5_7,mlx5_10,mlx5_11}"
    export NCCL_IB_GID_INDEX="${NCCL_IB_GID_INDEX:-3}"
    export NCCL_NET_GDR_LEVEL="${NCCL_NET_GDR_LEVEL:-5}"

    # --- Performance tuning ---
    export NCCL_BUFFSIZE="${NCCL_BUFFSIZE:-2097152}"
    export LD_LIBRARY_PATH="/usr/local/lib/:${LD_LIBRARY_PATH:-}"

    # --- TorchTitan repository setup ---
    TORCHTITAN_REPO="${TORCHTITAN_REPO:-/workspace/torchtitan}"
    if [[ ! -d "$TORCHTITAN_REPO" ]]; then
      echo "[INFO] Cloning TorchTitan repository to ${TORCHTITAN_REPO}..."
      git clone https://github.com/pytorch/torchtitan.git "$TORCHTITAN_REPO"
    fi

    # --- TorchTitan config ---
    CONFIG_FILE="${CONFIG_FILE:-${TORCHTITAN_REPO}/train_configs/llama3_8b.toml}"

    # --- Distributed parameters ---
    NNODES="${NNODES:-2}"
    NPROC_PER_NODE="${NPROC_PER_NODE:-4}"

    # Extract node rank from pod name
    POD_NAME="${HOSTNAME}"
    NODE_RANK="${POD_NAME##*-}"

    # Rendezvous settings
    MASTER_ADDR="${MASTER_ADDR:-h-kim-torchtitan-0.h-kim-torchtitan-headless.nccl-test.svc.cluster.local}"
    MASTER_PORT="${MASTER_PORT:-29500}"
    RDZV_ENDPOINT="${MASTER_ADDR}:${MASTER_PORT}"

    echo "=========================================="
    echo "TorchTitan Distributed Training - OpenShift"
    echo "=========================================="
    echo "[INFO] pod=${POD_NAME}"
    echo "[INFO] node_rank=${NODE_RANK}"
    echo "[INFO] nnodes=${NNODES}"
    echo "[INFO] nproc_per_node=${NPROC_PER_NODE}"
    echo "[INFO] total_gpus=$((NNODES * NPROC_PER_NODE))"
    echo "[INFO] rdzv_endpoint=${RDZV_ENDPOINT}"
    echo "[INFO] config_file=${CONFIG_FILE}"
    echo "=========================================="
    echo ""

    # Move to TorchTitan repo
    cd "$TORCHTITAN_REPO"

    # Show GPU info
    echo "[INFO] Available GPUs:"
    python3 -c "import torch; [print(f'  GPU {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"
    echo ""

    # --- Run TorchTitan training ---
    echo "[INFO] Starting torchrun..."
    exec torchrun \
      --nnodes="${NNODES}" \
      --nproc_per_node="${NPROC_PER_NODE}" \
      --node_rank="${NODE_RANK}" \
      --rdzv_backend=c10d \
      --rdzv_endpoint="${RDZV_ENDPOINT}" \
      --rdzv_id="${JOB_ID:-1}" \
      --max_restarts=3 \
      train.py --job.config_file "${CONFIG_FILE}" "$@"
---
apiVersion: v1
kind: Service
metadata:
  name: h-kim-torchtitan-headless
  namespace: nccl-test
spec:
  clusterIP: None
  selector:
    app: h-kim-torchtitan
  ports:
  - port: 29500
    name: master
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: h-kim-torchtitan
  namespace: nccl-test
  labels:
    app: h-kim-torchtitan
spec:
  serviceName: h-kim-torchtitan-headless
  replicas: 2  # 2 nodes Ã— 4 GPUs = 8 total
  podManagementPolicy: Parallel

  selector:
    matchLabels:
      app: h-kim-torchtitan

  template:
    metadata:
      labels:
        app: h-kim-torchtitan

    spec:
      restartPolicy: Always

      # Spread pods across different nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - h-kim-torchtitan
            topologyKey: kubernetes.io/hostname

      nodeSelector:
        nvidia.com/gpu.present: "true"

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      containers:
      - name: h-kim
        image: quay.io/jschless/ml-dev-env:h-kim
        imagePullPolicy: Always

        resources:
          requests:
            nvidia.com/gpu: 4
            memory: 128Gi
            cpu: 32
          limits:
            nvidia.com/gpu: 4
            memory: 256Gi
            cpu: 64

        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"

        # NCCL configuration
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_IB_DISABLE
          value: "0"
        - name: NCCL_IB_HCA
          value: "mlx5_6,mlx5_7,mlx5_10,mlx5_11"
        - name: NCCL_IB_GID_INDEX
          value: "3"
        - name: NCCL_NET_GDR_LEVEL
          value: "5"
        - name: NCCL_SOCKET_IFNAME
          value: "net1,net2,net3,net4"

        # Distributed training settings
        - name: MASTER_ADDR
          value: "h-kim-torchtitan-0.h-kim-torchtitan-headless.nccl-test.svc.cluster.local"
        - name: MASTER_PORT
          value: "29500"
        - name: NNODES
          value: "2"
        - name: NPROC_PER_NODE
          value: "4"

        # TorchTitan config (optional - can override)
        # - name: CONFIG_FILE
        #   value: "/workspace/torchtitan/train_configs/llama3_8b.toml"

        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: dshm
          mountPath: /dev/shm
        - name: training-script
          mountPath: /scripts
          readOnly: true

        command:
        - /bin/bash
        - -c
        - |
          set -e

          # Copy script to workspace and make executable
          cp /scripts/h-kim-openshift.sh /workspace/
          chmod +x /workspace/h-kim-openshift.sh

          # Wait a bit for all pods to be ready (especially if this is pod 1)
          POD_ORDINAL=${HOSTNAME##*-}
          if [ "$POD_ORDINAL" != "0" ]; then
            echo "Waiting 10s for master pod to initialize..."
            sleep 10
          fi

          # Run the training script
          echo "Starting TorchTitan training..."
          exec /workspace/h-kim-openshift.sh

      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi
      - name: training-script
        configMap:
          name: h-kim-training-script
          defaultMode: 0755

  # Each pod gets its own workspace PVC
  volumeClaimTemplates:
  - metadata:
      name: workspace
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
      storageClassName: ocs-external-storagecluster-ceph-rbd
