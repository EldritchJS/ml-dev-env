apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: ml-dev-env
  namespace: nccl-test
spec:
  # Increase build resources if build fails with OOM
  resources:
    limits:
      cpu: "8"
      memory: "16Gi"
    requests:
      cpu: "4"
      memory: "8Gi"

  output:
    to:
      kind: ImageStreamTag
      name: ml-dev-env:latest

  source:
    type: Dockerfile
    dockerfile: |
      # Multi-stage build for ML development environment with all required tools
      FROM nvcr.io/nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base

      # Set environment variables
      ENV DEBIAN_FRONTEND=noninteractive \
          PYTHONUNBUFFERED=1 \
          CUDA_HOME=/usr/local/cuda \
          PATH=/usr/local/cuda/bin:$PATH \
          LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

      # Install system dependencies including RDMA/InfiniBand tools
      RUN apt-get update && apt-get install -y \
          python3.10 \
          python3.10-dev \
          python3-pip \
          git \
          wget \
          curl \
          vim \
          less \
          bash-completion \
          build-essential \
          cmake \
          ninja-build \
          ffmpeg \
          libsm6 \
          libxext6 \
          libxrender-dev \
          libgomp1 \
          infiniband-diags \
          ibverbs-utils \
          rdma-core \
          libibverbs-dev \
          librdmacm-dev \
          pciutils \
          kmod \
          numactl \
          && rm -rf /var/lib/apt/lists/*

      # Create python3 symlink
      RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1 && \
          update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

      # Upgrade pip and install wheel
      RUN pip install --no-cache-dir --upgrade pip setuptools wheel

      # Install PyTorch with CUDA support
      RUN pip install --no-cache-dir \
          torch==2.1.2 \
          torchvision==0.16.2 \
          torchaudio==2.1.2 \
          --index-url https://download.pytorch.org/whl/cu121

      # Set NCCL environment variables for RDMA/GPUDirect
      ENV NCCL_DEBUG=INFO \
          NCCL_IB_DISABLE=0 \
          NCCL_IB_HCA=mlx5 \
          NCCL_IB_GID_INDEX=3 \
          NCCL_SOCKET_IFNAME=eth0 \
          NCCL_NET_GDR_LEVEL=5

      # Install flash-attention (requires CUDA)
      RUN pip install --no-cache-dir flash-attn --no-build-isolation

      # Install transformers and related libraries
      RUN pip install --no-cache-dir \
          transformers>=4.37.0 \
          accelerate \
          datasets \
          tokenizers \
          sentencepiece \
          protobuf

      # Install deepspeed
      RUN pip install --no-cache-dir deepspeed

      # Install LLaMAFactory and its dependencies
      RUN pip install --no-cache-dir \
          llamafactory \
          peft \
          trl \
          bitsandbytes

      # Install VideoLLaMA2 dependencies
      RUN pip install --no-cache-dir \
          einops \
          timm \
          av \
          opencv-python \
          decord

      # Install EasyR1 and additional ML tools
      RUN pip install --no-cache-dir \
          scipy \
          scikit-learn \
          matplotlib \
          jupyter \
          ipykernel \
          notebook

      # Install development tools
      RUN pip install --no-cache-dir \
          ipython \
          debugpy \
          pytest \
          black \
          flake8 \
          tensorboard \
          wandb

      # Install code-server (VSCode in browser)
      RUN curl -fsSL https://code-server.dev/install.sh | sh

      # Create workspace directory
      RUN mkdir -p /workspace

      # Configure bash completion
      RUN echo "if [ -f /usr/share/bash-completion/bash_completion ]; then" >> /root/.bashrc && \
          echo "  source /usr/share/bash-completion/bash_completion" >> /root/.bashrc && \
          echo "fi" >> /root/.bashrc && \
          echo "export PS1='\\[\\033[01;32m\\]\\u@ml-dev\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '" >> /root/.bashrc && \
          echo "alias ll='ls -lah'" >> /root/.bashrc

      # Set working directory
      WORKDIR /workspace

      # Expose ports for code-server, jupyter, tensorboard
      EXPOSE 8080 8888 6006

      # Create entrypoint script
      RUN echo '#!/bin/bash' > /entrypoint.sh && \
          echo 'echo "=========================================="' >> /entrypoint.sh && \
          echo 'echo "ML Development Environment"' >> /entrypoint.sh && \
          echo 'echo "=========================================="' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "GPU Information:"' >> /entrypoint.sh && \
          echo 'nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv,noheader' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "GPU Topology:"' >> /entrypoint.sh && \
          echo 'nvidia-smi topo -m' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "InfiniBand Devices:"' >> /entrypoint.sh && \
          echo 'ibstat 2>/dev/null || echo "  No IB devices found (may need host network)"' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "Python version:"' >> /entrypoint.sh && \
          echo 'python --version' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "PyTorch version:"' >> /entrypoint.sh && \
          echo 'python -c "import torch; print(f\"PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}, CUDA version: {torch.version.cuda}\")"' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "Installed packages:"' >> /entrypoint.sh && \
          echo 'echo "  - transformers: $(python -c \"import transformers; print(transformers.__version__)\")"' >> /entrypoint.sh && \
          echo 'echo "  - deepspeed: $(python -c \"import deepspeed; print(deepspeed.__version__)\")"' >> /entrypoint.sh && \
          echo 'echo "  - flash-attn: installed"' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'echo "=========================================="' >> /entrypoint.sh && \
          echo 'echo "VSCode Server: http://localhost:8080"' >> /entrypoint.sh && \
          echo 'echo "Jupyter: http://localhost:8888"' >> /entrypoint.sh && \
          echo 'echo "TensorBoard: http://localhost:6006"' >> /entrypoint.sh && \
          echo 'echo "=========================================="' >> /entrypoint.sh && \
          echo 'echo ""' >> /entrypoint.sh && \
          echo 'exec "$@"' >> /entrypoint.sh && \
          chmod +x /entrypoint.sh

      ENTRYPOINT ["/entrypoint.sh"]
      CMD ["tail", "-f", "/dev/null"]

  strategy:
    type: Docker
    dockerStrategy:
      dockerfilePath: Dockerfile

  # Automatically rebuild on config change
  triggers:
    - type: ConfigChange
